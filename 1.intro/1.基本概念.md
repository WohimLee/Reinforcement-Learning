## 强化学习的基本元素

#### Agent（智能体）
- 执行动作（Action）并从环境中获取反馈
- 学习如何在不同状态下采取最佳行动

#### Environment（环境）
- 智能体所处的外部系统
- 接收智能体的动作并返回新的状态（State）和奖励（Reward）

#### State（状态）

- 环境在某一时刻的具体描述，是智能体决策的依据
- 用 $s_t$ 表示第 $t$ 时刻的状态

#### Action（动作）

- 智能体在状态 $s_t$ 下可以选择的行为，用 $a_t$ 表示
- 不同的动作会导致不同的状态转移

#### Reward（奖励）

- 环境对智能体行动的反馈，用 $r_t$ 表示
- 奖励值衡量当前动作的好坏，引导智能体学习

#### Policy（策略）

- 从状态到动作的映射关系： $\pi(a|s)$
- 决定智能体在特定状态下采取哪个动作

#### Value Function（价值函数）
- 衡量某个状态或状态-动作对的“长期收益”。

主要有：

>状态价值函数 

$$
V^\pi(s)=\mathbb{E}\left[R_t \mid s_t=s\right]
$$

>动作价值函数 

$$
Q^\pi(s, a)=\mathbb{E}\left[R_t \mid s_t=s, a_t=a\right]
$$

#### Model（模型，可选）
- 描述环境动态：状态转移概率和奖励函数。