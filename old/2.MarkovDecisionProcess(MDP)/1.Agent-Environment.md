&emsp;
# Agent-Environment
# 1 引例
>网格问题

<div align=center><image src="imgs/grid_world.png" width=500></div>

- Actions: 东、南、西、北；每个 Action 的 Reward=0；Action 的概率相等
- 有几个特殊的情况
    - 如果采取的 Action 后，不在网格内，那么位置不变，Reward=-1
    - 在 State 状态 A 中，4 种 Action 的 Reward=10，并且 Agent 移动到 A'
    - 在 State 状态 B 种，4 种 Action 的 Reward=5，并且 Agent 移动到 B'

&emsp;
# 2 MDP
>马尔科夫决策过程（MDP）
- Agent: 智能体
- Environment: 环境
<div align=center>
    <image src="./imgs/agent-environment.png" width=500>
</div>

$$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3, ...$$

- $t$: $t=0, 1, 2, 3, ...$ Agent 和 Environment 发生交互的 `离散时刻`

- $S_t \in \mathcal{S}$: Agent 体观察到所在的 `Environment 状态的某种特征表达`
- $A_t \in \mathcal{A}(s)$: 在 $S_t$ 下选择的一个动作
- $R_{t+1} \in \mathcal{R} \subset \mathbb{R}$: 下一时刻，作为 $A_t$ 的结果，Agent 接收到一个数值化的收益（Reward），并进入到一个新的状态 $S_{t+1}$

&emsp;
>有限马尔科夫
- 在有限 MDP 中，State、Action 和 Reward的集合（$\mathcal{S}$、$\mathcal{A}$、$\mathcal{R}$）都只有有限个元素
- 随机变量 $R_t$ 和 $S_t$ 具有定义明确的离散概率分布
- 随机变量 $R_t$ 和 $S_t$ 只以来与前继状态和动作
- 综上，给定前继状态和动作的值时，这些随机变量的特定值，$s' \in \mathcal{S}$ 和 $r \in \mathcal{R}$ 在 t 时刻出现的概率是：
$$p(s', r | s, a) \doteq P\{S_t = s', R_t=r | S_{t-1}=s, A_{t-1}a\}$$


&emsp;
>马尔科夫性
- State 必须包括过去 Agent 和 Environment 交互的方方面面的信息，这些信息会对未来产生一定影响
- 这样的状态就被认为具有 `马尔科夫性`


&emsp;
# 3 实现

```py
import time
import random
import numpy as np

WORLD_SIZE = 5
A_POS = [0, 1]
A_PRIME_POS = [4, 1]
B_POS = [0, 3]
B_PRIME_POS = [2, 3]
DISCOUNT = 0.9

# left, up, right, down
ACTIONS = [
    np.array([0, -1]), 
    np.array([-1, 0]), 
    np.array([0, 1]), 
    np.array([1, 0])
]

NUM_ACTIONS = len(ACTIONS)

ACTIONS_FIGS=[ '←', '↑', '→', '↓']

ACTION_PROB = 0.25


def print_before_step(state, action_idx):
    if state == A_POS:
        print(f"Current State: A, Action: {ACTIONS_FIGS[action_idx]}")
    elif state == B_POS:
        print(f"Current State: B, Action: {ACTIONS_FIGS[action_idx]}")
    else:
        print(f"Current State: {state}, Action: {ACTIONS_FIGS[action_idx]}")
            

def print_after_step(state, reward):
    if state == A_PRIME_POS:
        print(f"Jump to State: A', Reward: {reward}")
    elif state == B_PRIME_POS:
        print(f"Jump to State: B', Reward: {reward}")
    else:
        print(f"Jump to State: {state}, Reward: {reward}")


def step(state, action):
    if state == A_POS:
        return A_PRIME_POS, 10
    if state == B_POS:
        return B_PRIME_POS, 5
    next_state = (np.array(state) + action).tolist()
    x, y = next_state
    if x < 0 or y < 0 or x >WORLD_SIZE or y > WORLD_SIZE:
        reward = -1.
        return state, reward
    else:
        reward = 0
        return next_state, reward
    
def algorithm():
    state = [0,0]
    cumulative_reward = 0.
    i=0
    while True:
        i+=1
        action_idx = np.random.randint(NUM_ACTIONS)
        action = ACTIONS[action_idx]
        print_before_step(state, action_idx)
        
        state, reward = step(state, action)
        print_after_step(state, reward)
        
        # cumulative_reward += reward
        discount_rate = pow(DISCOUNT, i)
        cumulative_reward += reward*discount_rate
        print(f"Cumulative Reward = {reward} * {discount_rate:.2f}",
                               f" = {cumulative_reward:.2f}\n")
        
        time.sleep(2)

        
if __name__ == '__main__':
    algorithm()

```