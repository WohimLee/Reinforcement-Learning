

&emsp;
# Action-Value function for policy $\pi$
>Action-Value 定义
- 从一个 State 出发，选择一个 Action 得到的 Average Reward

    $$q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t | S_t = s, A_t=a] $$
    - $q_{\pi}(s,a)$ 依赖 $\pi$


&emsp;
>包含 $q_{\pi}$ 的 $v_{\pi}$表达式
$$v_{\pi}(s) = \mathbb{E}\Big[G_t | S_t=s \Big] $$
$$\qquad\qquad\qquad\qquad\quad\ \ = \sum\limits_a \Big[ G_t | S_t=s, A_t=a\Big]\pi(a|s)$$

$$\quad\ \ \ v_{\pi}(s) = \sum\limits_a \pi(a|s)q_{\pi}(s|a)$$
- 这个式子表示：
    - 已知：一个 State 所有的 Action Value 
    - 求平均得到这个 State 的 State Value

&emsp;
>$q_{\pi}$ 的另一种表达式
- 回顾贝尔曼公式
    <div align=center><image src="imgs/bellmanEquation2.png" width=500></div>

    - `[]` 内的就是 $q_{\pi}(s,a)$

- 所以我们就得到了 Action-Value 的另外一种表达式：
    $$q_{\pi}(s,a)=\sum\limits_r p(r|s,a)r + \gamma\sum\limits_{s'} p(s' | s,a)\ v_{\pi}(s')$$
    - 这个式子表示
        - 已知：所有 State 的 State Value
        - 可以求出所有 Action 的 Action Value


&emsp;
# 讨论

当 Environment 中有很多 State 时，独立的估算每个 State 的平均值是不切实际的。这种情况下，我们可以将价值函数 $v_{\pi}$ 和 $q_{\pi}$ 进行参数化，然后通过调整 $v_{\pi}$ 的参数来更好的计算回报值
