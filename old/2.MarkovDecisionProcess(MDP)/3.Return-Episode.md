
&emsp;
# Returns and Episode
>Returns（回报）
- 一般来说，我们寻求的是最大化期望回报，记为 $G_t$，它被定义为收益序列的一些特定函数，在最简单的情况下，Return 是 Reward 的总和:
    $$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + ...  + R_T$$
    - $T$: 每一幕（Episode）的最终时刻、终结时刻

&emsp;
>Episode（分幕式任务）
- Agent 和 Environment 的交互过程中的一系列子序列
- 例如：训练多盘游戏中的一盘游戏，训练多次迷宫中的一次走迷宫
- $\mathcal{S}$: 非终结状态集
- $\mathcal{S}^+$: 包含终结与非终结状态的所有状态集


&emsp;
>持续性任务
- 如果 Agent-Environment 的交互过程不是能被分开的 Episode，而是持续不断的发生，那么上面的回报公式：
    $$G_t \doteq R_{t+1} + R_{t+2} + R_{t+3} + ...  + R_T$$
    会因为 $T=\infty$ 而 $G_t$ 趋于无穷

- 这样就需要引入 Discount（折扣）的概念


&emsp;
>Discount 折扣
- 根据这种方式，Agent 尝试选择 Action，使得经过折扣系数加权后的收益总和最大化
    $$G_t \doteq \gamma^0R_{t+1} + \gamma^1 R_{t+2} + \gamma^2R_{t+3} + ... = \sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}$$
    - $\gamma$: 折扣率，$0 \leq \gamma \leq 1$
        - $\gamma < 1$:
        - $\gamma=0$: Agent 是 "目光短浅的"，只关心最大化当前收益， 那么当前某个时刻 $t$ 下，就有可能选择 $A_t$ 来最大化 $R_{t+1}$。然而，最大化当前收益一般会减少未来收益
        - $\gamma$ 接近1: 更多的考虑未来收益，更 "有远见"
    - 邻近时刻的 Return 可以由上式推导为：
    $$G_t \doteq R_{t+1} + \gamma G_{t+1}$$


&emsp;
>分幕式&持续性任务统一表示法
- $S_{t, i}$: 表示 Episode（幕） $i$ 中时刻 $t$ 的 State，同理（$R_{i,t}、\pi_{t, i}、T_i$）