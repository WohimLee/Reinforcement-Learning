&emsp;
# Prerequisites

# 1 符号约定
>基础符号

- 在 $t$ 时刻（或者说 step）：
    - $A_t$: 选择的动作（Action）
    - $q_*(a)$: 给定 Action 时收益的期望
    $$q_*(a) \doteq \mathbb{E}[R_t | A_t=a] $$
    - $R_t$: $t$ 时刻选择的 Action 对应的收益（Reword）
    - $Q_t(a)$: 某个 Action $a$ 在被执行多次（step 或者 time）之后，在时刻 $t$ 时的价值的估计，我们希望它接近 $q_*(a)$

>$Q_{n+1}$

$$Q_n \doteq \frac{R_1 + R_2 + ... + R_{n-1}}{n-1} $$

- $R_i$: 一个 Action 被选择 $i$ 次后获得的收益
- $Q_n$: 这个 Action 被选择 $n-1$ 次后它的估计 Action-Value

将上面式子推导一下

$$Q_{n+1} = \frac{1}{n}\sum\limits_{i=1}^{n}R_i$$
$$=\frac{1}{n}\Big(R_n+\sum\limits_{i=1}^{n-1}R_i\Big)$$
$$=\frac{1}{n}\Big(R_n+(n-1)\frac{1}{n-1}\sum\limits_{i=1}^{n-1}R_i\Big)$$
$$=\frac{1}{n}\Big(R_n + (n-1)Q_n\Big)$$
$$=\frac{1}{n}\Big(R_n + nQ_n - Q_n\Big)$$
$$=Q_n+\frac{1}{n}\Big[R_n - Q_n\Big]$$


>Action estimation 更新公式
- 新估计值 $\leftarrow$ 旧估计值 + 步长 $\times$ [目标-旧估计值]
    - \[目标-旧估计值]: 是估计值的误差

&emsp;
>老虎机的例子
- 假设现在有一台 10 个按钮的老虎机
    - $A_t$: 分别对应 10 个按钮
    - $q_*(a)$: 10 个按钮分别对应的收益的期望
    - $R_t$: 某个时刻按下一个按钮对应的实际收益
    - $Q_t(a)$: 某个按钮被按下多次之后，计算出的估计收益值，我们希望它接近 $q_*(a)$


&emsp;
# 2 代码理解
## 2.1 对应的符号
我们以一个有 10 个按钮的老虎机为例子

>$A_t$
- 10 个按钮：0, 1, 2, 3, 4, 5, 6, 7, 8, 9
    ```py
    indices = np.arange(10)
    ```
>$q_*(a)$
- 我们随机（不一定是随机，比如说一些黑心庄家）生成 10 个数字作为 10 个 Action 的 Reward 的期望，这 10 个数字有正有负，代表赢钱和输钱
    ```py
    q_expectation = np.random.randn(10)
    ```
>$R_t$
- 然后在这 10 个数字的基础加上高斯分布的噪声，代表这 10 个 Reward 它只是期望，而不是百分比的确定值
    ```py
    reword = np.random.randn(10) + np.random.randn(10)
    ```
>$Q_{t}(a)$
- 我们做 200 次这样的实验统计数据
    ```py
    np.random.randn(200, 10) + np.random.randn(10)
    ```

&emsp;
## 2.2 可视化
```py
import numpy as np
import matplotlib.pyplot as plt


def distribution_expectation():
    plt.violinplot(dataset=np.random.randn(200, 10) + np.random.randn(10), showmeans=True)
    plt.xlabel("Action")
    plt.ylabel("Reward distribution")
    plt.savefig('../imgs/action-reward_distribution.png')
    plt.close()
    
distribution_expectation()
```

<div align=center>
    <image src="imgs/action-reward_distribution.png" width=800>
</div>
&emsp;

上面的期望值计算如下：
$$Q_{n+1}=Q_n+\frac{1}{n}\Big[R_n - Q_n\Big]$$
```py
q_estimation[action] += (reward - q_estimation[action]) / action_count[action]
```