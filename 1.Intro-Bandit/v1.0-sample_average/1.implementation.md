
# 1 实验目的
我们将做下面的实验：
- 3 种贪心算法：
    - $\epsilon = 0$: 只选择 estimation 收益最大的 Action
    - $\epsilon = 0.1$: 有 10% 的概率会尝试新的 Action
    - $\epsilon = 0.01$: 有 1% 的概率会尝试新的 Action
- 每种贪心算法进行 2000 轮（run），每轮都是不同的 Action-Value
- 每个 run 有 1000 次（time）

最后做一下评估：
- 3 种贪心算法在 1000 次中，2000 种不同 Action-Value 下的表现

&emsp;
# 2 实验过程
## 2.1 创建算法
用 list 存储 3 种不同的 $\epsilon$，创建对应的 3 种 Bandit 实例
```py
def greedy_algorithm(runs=2000, steps=1000):
    # 贪心算法、0.1-贪心算法、0.01-贪心算法
    epsilons = [0, 0.1, 0.01]
    bandits = [Bandit(epsilon=eps) for eps in epsilons]
    ...
```

&emsp;
## 2.1 建立 Bandit 模型
对应的每一种贪心算法，我们创建一个 Bandit 的示例对象
>class Bandit
```py
bandits = [Bandit(epsilon=eps) for eps in epsilons]

class Bandit:
    def __init__(self, k_arm=10, epsilon=0.):
        self.k = k_arm
        self.epsilon = epsilon
        # 每个 Action 的索引
        self.indices = np.arange(self.k)
        self.step = 0
    ...
```

&emsp;
## 2.3 模拟算法过程
通过 simulate 函数模拟过程得到最后的结果，返回值有两个：
- 每个 step 的 2000 个 run 中，选择最优 Action 的比例
- 每个 step 的 2000 个 run 的平均 Reward

>simulate()
```py
def greedy_algorithm(runs=2000, steps=1000):
    ...
    best_action_rates, mean_rewards = simulate(runs, steps, bandits)
    ...

def simulate(runs, step, bandits):
    rewards = np.zeros((len(bandits), runs, step))
    best_action_counts = np.zeros(rewards.shape)
    for i, bandit in enumerate(bandits):
        for r in trange(runs):
            bandit.reset()
            for t in range(step):
                action = bandit.act()
                reward = bandit.step(action)
                rewards[i, r, t] = reward
                if action == bandit.best_action:
                    best_action_counts[i, r, t] = 1
    # 每个不同 epsilon 的 Bandit 中，每个 step，
    # 在 2000 个 run 中采取 best Action 的概率
    best_action_rates = best_action_counts.mean(axis=1)
    # 每个不同 epsilon 的 Bandit 中，每个 step，
    # 2000 个 run 的平均 Reward
    mean_rewards = rewards.mean(axis=1)
    return best_action_rates, mean_rewards
```

完善 Bandit 的 reset()、act()、step() 函数

>class Bandit
```py
class Bandit:
    ...
    def reset(self):
        '''
        功能：用于每个 Run 重置数据
        '''
        # 每一个 Run 重置 q_true
        self.q_true = np.random.randn(self.k)
        # 每个 Action 的估计值存储空间，shape=(10,)
        self.q_estimation = np.zeros(self.k)
        # 每个 Action 被选择的次数
        self.action_count = np.zeros(self.k)
        # Reward 最高的 Action 的索引
        self.best_action = np.argmax(self.q_true)
        # step 重置为 0
        self.step = 0

    def act(self):
        '''
        功能：获取一个 Action
        '''
        # epsilon*100% 的概率尝试新的 Action
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.indices)

        # 否则就选估计 Reward 最大的 Action
        q_best = np.max(self.q_estimation)
        # 如果最大的 Reward 值有好几个，随机选一个
        return np.random.choice(np.where(self.q_estimation == q_best)[0])

    # take an action, update estimation for this action
    def step(self, action):
        '''
        功能：执行一个 Action，并且更新这个 Action 的 Reward
        '''
        # 因为 reward 不是确定的，而是有一定概率分布
        # 所以这里模拟老虎机实际出来的 reward
        reward = np.random.randn() + self.q_true[action]
        self.step += 1
        self.action_count[action] += 1


        # update estimation using sample averages
        self.q_estimation[action] += (reward - self.q_estimation[action]) \
                                    / self.action_count[action]
        return reward
```

&emsp;
## 2.4 绘制结果
```py
def greedy_algorithm(runs=2000, steps=1000):
    ...
    plt.figure(figsize=(10, 20))

    plt.subplot(2, 1, 1)
    for eps, rewards in zip(epsilons, mean_rewards):
        plt.plot(rewards, label='$\epsilon = %.02f$' % (eps))
    plt.xlabel('steps')
    plt.ylabel('average reward')
    plt.legend()

    plt.subplot(2, 1, 2)
    for eps, rate in zip(epsilons, best_action_rates):
        plt.plot(rate*100, label='$\epsilon = %.02f$' % (eps))
    plt.xlabel('steps')
    plt.ylabel('Optimal Action (%)')
    plt.legend()

    plt.savefig('imgs/bandit-1.0.png')
    plt.close()
```






















