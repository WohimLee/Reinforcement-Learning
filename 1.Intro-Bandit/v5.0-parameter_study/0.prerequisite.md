&emsp;
# Prerequisite
>老虎机小结
- 在选择 Action 方面：
    - $0$-贪心算法: $A_t = \argmax\limits Q_t(a)$

    - $\epsilon$-贪心算法: 在上面的基础上，一定的概率随机选择 Action
    - UCB: $A_t = \argmax\limits \Big[Q_t(a) + c\sqrt{\frac{ln t}{N_t(a)}} \Big]$

    - gradient: `np.random.choice(indices, p=softmax(`$Q_t(a)$`))`

- 在更新公式方面：
    - sample average: $Q_{n+1}=Q_n+\frac{1}{n}\Big[R_n - Q_n\Big]$
    
    - $\alpha$ 步长: $Q_{n+1} = Q_n + \alpha[R_n-Q_n]$

    - gradient: $Q_{t+1}(a) = Q_t(a) + \alpha(R_t - \bar{R}_t)(\mathbb{I}_{a=A_t} - softmax)$


我们将对 4 种方法使用不同的参数运行
- 横向对比：同一种算法，不同参数下 2000 个 run，1000 个 time 的平均收益（表述可能有点问题，建议直接看代码）
- 纵向对比：不同算法之间的平均收益
- 因为它们都是凸函数，所以下面选择的参数都是实验过的（不信的自己去跑跑看）

```py
def algorithm(runs=2000, time=1000):
    labels = ['$\epsilon$-greedy', 'gradient bandit',
              'UCB', 'optimistic initialization']
    generators = [lambda epsilon: Bandit(epsilon=epsilon, sample_averages=True),
                  lambda alpha: Bandit(gradient=True, step_size=alpha, gradient_baseline=True),
                  lambda coef: Bandit(epsilon=0, UCB_param=coef, sample_averages=True),
                  lambda initial: Bandit(epsilon=0, initial=initial, step_size=0.1)]
    parameters = [np.arange(-7, -1, dtype=np.float32),
                  np.arange(-5, 2, dtype=np.float32),
                  np.arange(-4, 3, dtype=np.float32),
                  np.arange(-2, 3, dtype=np.float32)]

    bandits = []
    for generator, parameter in zip(generators, parameters):
        for param in parameter:
            bandits.append(generator(pow(2, param)))
```

&emsp;
## 1 $\epsilon$-贪心算法
- $\epsilon$-贪心算法 + sample average
- np.arange(-7, -1, dtype=np.float32): 6 个参数

&emsp;
## 2 $\alpha$
- gradient + $\alpha$ + gradient baseline
- np.arange(-5, 2, dtype=np.float32): 7 个参数

&emsp;
## 3 UCB
- 0-贪心算法 + UCB + sample average
- np.arange(-4, 3, dtype=np.float32): 7 个参数


&emsp;
## 4 initial
- 0-贪心算法 + initial + $al$
- np.arange(-2, 3, dtype=np.float32): 5 个参数


