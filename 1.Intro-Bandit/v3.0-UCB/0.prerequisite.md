&emsp;
# Prerequisite

v1.0 中 $\epsilon$-贪心算法有一定的概率尝试其它 Action，但是这是一种盲目的选择（np.choice）
```py
...
if np.random.rand() < self.epsilon:
    return np.random.choice(self.indices)
...
```
这个尝试的过程，它是所有 Action 随机选择，并不考虑有选择最大 Reward 的 Action，或者说一些从来没尝试过的 Action。

我们其实应该根据它们的潜力来选择那些 "可能是最优" 的 Action, 这就要考虑两个问题：
- 哪个 Action 的 $Q_t(a)$ 比较接近最大的 Reward
- Action 的 $Q_t(a)$ 的确定性
    - $Q_t(a)$ 确定性高：被选择多次，计算了多次 $Q_t(a)$
    - $Q_t(a)$ 确定性低：
        - 没被选过，仍然还是初始值
        - 选的次数低，计算的 $Q_t(a)$ 不算太准确（当然要多次采样计算才比较准确）


>Upper Confidence Bound(UCB)
- 针对上面所说的问题，比较有效的方法是按照以下公式选择动作：
$$A_t \doteq \argmax\limits_a\Big[Q_t(a) + c\sqrt{\frac{ln t}{N_t(a)}} \Big]$$
- []: 里面是新的一种更新 $Q_t(a)$ 的方式
- $\sqrt{}$: 平方根里面是一个不确定项
    - $N_t(a)$: 在 t 时刻前，动作（Action） a 被选择的次数
    - $t$: 是这个
- $c$: 置信度，也可以理解为对 $\sqrt{}$ 内不确定项的权重
- $ln t$ 是分子，$N_t(a)$ 是分母：
    - 选择某个动作（Action）$a$ 的次数越多, $N_t(a)$ 越大，不确定项变小；而那些被选次数少的 Action，不确定项就会相对的"大"，这样会鼓励算法去选择那些被选次数少的 Action
    - 总次数 $t$ 中，选择 $a$ 以外的次数越多，$N_t(a)$ 不变，$ln t$ 会增大，但是根据函数图像，它增长的速度会越来越慢，导致整个不确定项增长的慢，所以
