&emsp;
# Prerequisite

# 1 Upper Confidence Bounding(UCB)
v1.0 中 $\epsilon$-贪心算法有一定的概率尝试其它 Action，但是这是一种盲目的选择（np.choice）
```py
...
if np.random.rand() < self.epsilon:
    return np.random.choice(self.indices)
...
```
这个尝试的过程，它是所有 Action 随机选择，并不考虑有选择最大 Reward 的 Action，或者说一些从来没尝试过的 Action。

我们其实应该根据它们的潜力来选择那些 "可能是最优" 的 Action, 这就要考虑两个问题：
- 哪个 Action 的 $Q_t(a)$ 比较接近最大的 Reward
- Action 的 $Q_t(a)$ 的确定性
    - $Q_t(a)$ 确定性高：被选择多次，计算了多次 $Q_t(a)$
    - $Q_t(a)$ 确定性低：
        - 没被选过，仍然还是初始值
        - 选的次数低，计算的 $Q_t(a)$ 不算太准确（当然要多次采样计算才比较准确）


>Upper Confidence Bound(UCB)
- 针对上面所说的问题，比较有效的方法是按照以下公式选择动作：
$$A_t \doteq \argmax\limits_a\Big[Q_t(a) + c\sqrt{\frac{ln t}{N_t(a)}} \Big]$$
- []: 里面是新的一种更新 $Q_t(a)$ 的方式
- $\sqrt{}$: 平方根里面是一个不确定项
    - $N_t(a)$: 在 t 时刻前，动作（Action） a 被选择的次数
    - $lnt$: t 是这个 run 中执行的总次数，随着 t 增大，$lnt$ 也会增大，但是后面的增速会越来越慢
- $c$: 置信度，也可以理解为对 $\sqrt{}$ 内不确定项的权重

>分析
- $N_t(a)$：
    - $N_t(a)$ 越大：选择某个动作（Action）$a$ 的次数越多
        - 由于采样次数多，计算的估计值比较 "准确"
        - 由于是分母，不确定项会随次数增多而变 "小"
        - 更新的估计值也变 "小"
    - $N_t(a)$ 越小：选择某个动作（Action）$a$ 的次数比较少
        - 由于采样次数少，计算的估计值不准确
        - 由于是分母，不确定项就会相对的 "大"
        - 更新的估计值也相对的 "大"
    - 这样会鼓励算法去选择那些被选次数少的 Action
- $ln t$：对于某一个动作 a
    - 总次数 $t$ 中，选择 $a$ 以外的次数越多
        - $N_t(a)$ 不变，$ln t$ 会增大
        - 因为在分子，不确定项也增大
        - 更新的估计值也相对的 "大"
    - 同样会鼓励算法去选择那些被选次数少的 Action
- 这样就会使得所有的 Action 都将会被选中，但是这两种的动作被选的频率会比较低：
    - Reward 比较低的
    - 被选的次数多的
- 这样就会促使算法去选那些 Reward 比较高的 Action，同时保证所有的 Action 的估计 Reward: $Q_t(a)$ 都计算的比较准确

&emsp;
# 2 代码理解
先构建更新公式：
$$Q_{t+1} = Q_t(a) + c\sqrt{\frac{ln t}{N_t(a)}}$$
```py
UCB_estimation = self.q_estimation + \
                 self.UCB_param * \
                 np.sqrt(np.log(self.time + 1) / \
                (self.action_count + 1e-5))
```
选择最大 $Q_t(a)$ 的 Action:
$$A_t \doteq \argmax\limits(Q_{t+1})$$
```py
q_best = np.max(UCB_estimation)
```

完整的 act 函数
```py
class Bandit:
    ...
    def act(self):
        if np.random.rand() < self.epsilon:
            return np.random.choice(self.indices)
        
        elif self.UCB_param:
            UCB_estimation = self.q_estimation + \
                self.UCB_param * np.sqrt(np.log(self.time + 1) / \
                (self.action_count + 1e-5))
            q_best = np.max(UCB_estimation)
            return np.random.choice(np.where(UCB_estimation == q_best)[0])
        
        else:
            q_best = np.max(self.q_estimation)
            return np.random.choice(np.where(self.q_estimation == q_best)[0])
    ...
```