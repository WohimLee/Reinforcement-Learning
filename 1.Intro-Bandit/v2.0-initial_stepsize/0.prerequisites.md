&emsp;
# Prerequisites

# 1 步长问题（step size）



v1.0 版本中，每个 run 的收益概率分布在每个时刻（time）或者说每一步（step）是不变的，所以我们计算 $Q_n$ 的时候可以用下面这个公式：
$$Q_{n+1}=Q_n+\frac{1}{n}\Big[R_n - Q_n\Big]$$

考虑另一种更常见的情况——非平稳问题（Nonstationary Problem）：这种情况下，每个时刻（time）或者说每一步（step）的收益概率分布都在改变，那么这种情形下 ，给近期的收益赋予比过去很久的收益更高的权值就是一种合理的处理方式。最流行的方法之一是使用固定步长。

比如说，用于更新 $n-1$ 个过去 Reward 的均值 $Q_n$ 的更新规则可以改为：
$$Q_{n+1} \doteq Q_n + \alpha [R_n - Q_n]$$
- $\alpha \in (0,1]$


>指数近因加权平均
- 由上式推导，可以知道改成 $\alpha$ 本质上就是指数近因加权平均

$$Q_{n+1} = Q_n + \alpha[R_n-Q_n]$$
$$= \alpha R_n + (1-\alpha)Q_n$$
$$= \alpha R_n + (1-\alpha)[\alpha R_{n-1} + (1-\alpha)Q_{n-1}]$$
$$= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2Q_{n-1}$$
$$= \alpha R_n + (1-\alpha)\alpha R_{n-1} + (1-\alpha)^2R_{n-2} + ... + (1-\alpha)^{n-1}\alpha R_1 + (1-\alpha)^nQ_1$$
$$=(1-\alpha)^nQ_1 + \sum\limits_{i=1}^n\alpha(1- \alpha)^{n-i}R_i$$
- 权值 $(1-\alpha)^n + \sum_{i=1}^n\alpha(1- \alpha)^{n-i}=1$


&emsp;
# 2 初始值的问题（initial）
## 2.1 初始值为 0
v1.0 版本中，初始估计值为 0
```py
def reset(self):
    ...
    self.q_estimation = np.zeros(self.k)
    ...
```
从统计学的角度来看，这样是有偏差的:
- sample average: 对于采样平均法来说，当所有动作都至少被选择一次时，偏差才会小时
- step size: 对于指数近因加权平均，步长为常数 $\alpha$ 的情况，偏差会随时间减小，但不会消失

&emsp;
## 2.2 乐观初始值
我们选择一个比较大的数，比如说 5，并且使用 $\epsilon=0$ 和 $\epsilon=0.1$ 的贪心算法做比较
```py
def reset(self):
    ...
    # self.initial = 5
    self.q_estimation = np.zeros(self.k) + self.initial
    ...
```

<div align=center>
    <image src="imgs/res.png" width=500>
</div>
&emsp;

这种乐观的估计会鼓励 Action-Value 方法去试探，因为无论哪一种动作被选择，收益都比最开始的估计值要小
```py
def act(self):
    ...
    # 5 远大于随机生成的 0-1 的数
    # 所以这样会鼓励算法把 10 个按钮都试一遍
    q_best = np.max(self.q_estimation)
    return np.random.choice(np.where(self.q_estimation == q_best)[0])
```




