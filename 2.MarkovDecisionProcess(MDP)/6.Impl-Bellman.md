&emsp;
# Implementation 

# 1 网格问题

<div align=center><image src="imgs/grid_world.png" width=500></div>

- Actions: 东、南、西、北；每个 Action 的 Reward=0；Action 的概率相等
- 有几个特殊的情况
    - 如果采取的 Action 后，不在网格内，那么位置不变，Reward=-1
    - 在 State 状态 A 中，4 种 Action 的 Reward=10，并且 Agent 移动到 A'
    - 在 State 状态 B 种，4 种 Action 的 Reward=5，并且 Agent 移动到 B'

    ```py
    WORLD_SIZE = 5
    A_POS = [0, 1]
    A_PRIME_POS = [4, 1]
    B_POS = [0, 3]
    B_PRIME_POS = [2, 3]
    DISCOUNT = 0.9

    # a1=left, a2=up, a3=right, a4=down
    ACTIONS = [np.array([0, -1]),
            np.array([-1, 0]),
            np.array([0, 1]),
            np.array([1, 0])]
    ACTIONS_FIGS=[ '←', '↑', '→', '↓']


    ACTION_PROB = 0.25
    ```


&emsp;
# 2 Value Function
## 2.1 回顾简化版的 Bellman Equation
$$\pmb{v}_{\pi} = \pmb{r}_{\pi} + \gamma \pmb{P}_{\pi}\pmb{v}_{\pi}$$

$$(\pmb{I} - \gamma \pmb{P}_{\pi})\pmb{v}_{\pi} = \pmb{r}_{\pi}\quad 或\quad 
(\gamma \pmb{P}_{\pi} - \pmb{I})\pmb{v}_{\pi} = -\pmb{r}_{\pi}$$

- $v_{\pi} = [v_{\pi}(s_1), ..., v_{\pi}(s_n)]^T \in \mathbb{R}^n$ （待求项）

- $P_{\pi} \in \mathbb{R}^{n \times n}$: State Transition Matrix（shape=(n, n)）
    - $[P_{\pi}]_{ij}=p_{\pi}(s_j | s_i)$ 代表从 $s_i$ 跳到 $s_j$

    - $p_{\pi}(s' | s) = \sum\limits_a \pi(a|s)\ p(s' | s,a)$

- $r_{\pi} = [r_{\pi}(s_1), ..., r_{\pi}(s_n)]^T \in \mathbb{R}^n$
    - $r_{\pi}(s_i) = \sum\limits_{a}\pi(a|s_i)\ \sum\limits_r p(r | s_i, a)r$


&emsp;
## 2.2 具体运算   
>计算 $\pmb{r}_{\pi}$
$$r_{\pi} = \begin{bmatrix}r_{\pi}(s_0) \\ \vdots \\ r_{\pi}(s_{24})\end{bmatrix}$$
- $r_{\pi}(s_0) = \pi(a_0|s_0)\ p(r_0 | s_0, a_0)\ r_0+$

    $\qquad\qquad  \pi(a_1|s_0)\ p(r_1 | s_0, a_1)\ r_1 +$

    $\qquad\qquad  \pi(a_2|s_0)\ p(r_2 | s_0, a_2)\ r_2 +$

    $\qquad\qquad  \pi(a_3|s_0)\ p(r_3 | s_0, a_3)\ r_3 +$

    $\quad\quad\ \ \ \  = 0.25 * 1 * (-1) +$

    $\qquad\qquad 0.25 * 1 * (-1) +$

    $\qquad\qquad 0.25 * 1 * 0 +$

    $\qquad\qquad 0.25 * 1 * 0 +$
- 以此类推计算 $r_{\pi}(s_1), r_{\pi}(s_2), r_{\pi}(s_3)...$

    ```py
    def algorithm():
        ...
        r = np.zeros(WORLD_SIZE*WORLD_SIZE)
        for i in range(WORLD_SIZE):
            for j in range(WORLD_SIZE):
                state_curr = [i, j]
                state_curr_idx = np.ravel_multi_index(state_curr, (WORLD_SIZE, WORLD_SIZE))
                for action in ACTIONS:
                    state_next, reward = step(state_curr, action)
                    ...
                    r[state_curr_idx] += ACTION_PROB*reward
    ```

&emsp;
>计算 $P_{\pi}$

$$\begin{bmatrix}P_{\pi}[0,0] & P_{\pi}[0,1] & ... & P_{\pi}[0,24]\\ \\
\vdots & \vdots & & \vdots\\ \\
P_{\pi}[24,0] & P_{\pi}[24,1] & ... & P_{\pi}[24,24]
\end{bmatrix}$$

- $P_{\pi}[00] = p_{\pi}(s_0 | s_0) = \sum\limits_{a=1,2,3,4} p(s_0 | s_0,a)\pi(a|s_0) = 1*0.25 + 1*0.25 + 0*0.25 + 0*0.25$ 

- $P_{\pi}[01] = p_{\pi}(s_1 | s_0) = \sum\limits_{a=1,2,3,4} p(s_1 | s_0,a)\pi(a_|s_0) = 0*0.25 + 0*0.25 + 1*0.25 + 0*0.25 $

- $P_{\pi}[02] = p_{\pi}(s_2 | s_0) = \sum\limits_{a=1,2,3,4} p(s_2 | s_0,a)\pi(a_|s_0 = 0*0.25 + 0*0.25 + 0*0.25 + 0*0.25)$

- ...
    ```py
    def algorithm():
        ...
        P = np.zeros(I.shape)
        ...
        for i in range(WORLD_SIZE):
            for j in range(WORLD_SIZE):
                state_curr = [i, j]
                state_curr_idx = np.ravel_multi_index(state_curr, (WORLD_SIZE, WORLD_SIZE))
                for action in ACTIONS:
                    state_next, reward = step(state_curr, action)
                    state_next_idx = np.ravel_multi_index(state_next, (WORLD_SIZE, WORLD_SIZE))
                    
                    P[state_curr_idx, state_next_idx] += ACTION_PROB
                    ...
    ```





