
&emsp;
# Bellman Equation（贝尔曼方程）

## 知识回顾
由前面的 State-Value function 公式：
$$v_{\pi}(s) \doteq \mathbb{E}_{\pi}[G_t | S_t=s]$$
$$=\mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]$$
$$= \mathbb{E}[R_{t+1}  | S_t=s] + \gamma  \mathbb{E}[G_{t+1} | S_t = s]$$

这个式子可以分为两部分:
- Immediate reward: $\mathbb{E}\Big[R_{t+1}  | S_t=s\Big]$
- Future reward: $\mathbb{E}\Big[G_{t+1} | S_t = s\Big]$

&emsp;
# 1 Immediate reward
先计算 Immediate reward: $\mathbb{E}[R_{t+1}  | S_t=s]$

$$\mathbb{E}\Big[R_{t+1}  | S_t=s\Big] = \sum\limits_{a}\pi(a|s)\ \mathbb{E}\Big[R_{t+1} | S_t = s, A_t = a\Big]$$
$$\qquad\ \ \  = \sum\limits_{a}\pi(a|s)\ \sum\limits_r p(r | s, a)\ r$$
- $p(r | s, a)$: 这个值在我们描述的网格问题里面 $=1$，因为不存在一个 State 下采取一个 Action 会有多个 Reward

&emsp;
# 2 Future reward
- 再计算 Future reward: $\mathbb{E}\Big[G_{t+1} | S_t = s\Big]$
    - 从一个 State 跳到另一个 State 的情况有多种
    - 下一个 State 的 Return 为 $G_{t+1}$
    - 求它们的均值（期望）

    $$\mathbb{E}[G_{t+1} | S_t = s]= \sum\limits_{s'} p(s' | s)\ \mathbb{E}\Big[G_{t+1} | S_t = s, S_{t+1} = s'\Big]$$

- $t+1$ 时刻的 Return:$G_{t+1}$ 跟当前 $t$ 时刻的 State:$S_t=s$ 没有任何关系，所以可以直接去掉 $S_t=s$，得到下面的式子：

    $$\qquad\quad\ \  = \sum\limits_{s'} p(s' | s)\ \mathbb{E}\Big[G_{t+1} | S_{t+1} = s'\Big]$$
            
- 而 $t+1$ 时刻的 Return:$G_{t+1}$ 就是等于 $t+1$ 时刻的 State 的价值函数
    $$\ \ = \sum_{s'}p(s'|s) \ v_{\pi}(s')$$

    $$\qquad\qquad\quad\ \  = \sum_{s'}v_{\pi}(s')\sum_a \pi(a|s)\ p(s' | s,a)$$
    - $v_{\pi}(s')$: 下一时刻的 State Value
    - $\pi(a|s)$: 策略，$State=s$ 下选择 $Action=a$ 的概率
    - $p(s' | s,a)$: $State=s$ 下选择 $Action=a$ 跳到 $State=s'$, 在这个网格的例子里，这个值 $= 1$，因为没有哪个 Action 执行之后可能会跳到不同的 $State$

&emsp;
# 3 Bellman Equation
整合上面的式子，得到了贝尔曼公式

<div align=center><image src="imgs/bellmanEquation.png" width=500></div>

- $v_{\pi}(s)$: 当前 State Value
- $v_{\pi}(s')$: 下一时刻的 State Value


&emsp;
# 4 Matrix-Vector Form
>Bellman Equation
- 回顾贝尔曼公式
    <div align=center><image src="imgs/bellmanEquation2.png" width=500></div>
- 将这个公式做一下变换
    - $r_{\pi}(s) = \sum\limits_{a}\pi(a|s)\ \sum\limits_r p(r | s, a)r$

    - $p_{\pi}(s' | s) = \sum\limits_a \pi(a|s)\ p(s' | s,a)$
- 得到下面的这个公式（单个 State）
    <div align=center><image src="imgs/bellmanEquation3.png" width=310></div>

- 将这个公式拓展一下，$s_i（i=1,2,...,n）$ 表示不同的 State
    <div align=center><image src="imgs/bellmanEquation4.png" width=320></div>
- 将多个 State 合并在一起，那么就可以写成矩阵和向量形式：
    $$\pmb{v}_{\pi} = \pmb{r}_{\pi} + \gamma \pmb{P}_{\pi}\pmb{v}_{\pi}$$

    - $v_{\pi} = [v_{\pi}(s_1), ..., v_{\pi}(s_n)]^T \in \mathbb{R}^n$

    - $r_{\pi} = [r_{\pi}(s_1), ..., r_{\pi}(s_n)]^T \in \mathbb{R}^n$
    - $P_{\pi} \in \mathbb{R}^{n \times n}$（State Transition Matrix）
        - $[P_{\pi}]_{ij}=p_{\pi}(s_j | s_i)=\sum\limits_a \pi(a|s_i)\ p(s_j | s_i,a)$
        - 我们这个网格问题中 $p(s_j | s_i,a)=1$, 所以 $[P_{\pi}]_{ij}=p_{\pi}(s_j | s_i)=\sum\limits_a \pi(a|s_i)$
        - 代表从 $s_i$ 跳到 $s_j$ 的概率矩阵

&emsp;
>示例
<div align=center><image src="imgs/stateTmatrix.png" width=500></div>