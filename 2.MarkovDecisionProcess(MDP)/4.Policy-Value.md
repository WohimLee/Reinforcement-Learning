&emsp;
# Policy-Value

## 知识回顾
回顾前面讲到的单步 Step 过程：
$$S_t \stackrel{A_t}\longrightarrow R_{t+1}，S_{t+1}$$
- $t，t+1$: Agent 和 Environment 发生交互的 `离散时刻`
- $S_t$: $t$ 时刻的状态 State
- $A_t$: $S_t$ 状态下执行的一个 Action
- $R_{t+1}$: 选择 $A_t$ 后的 Reward
- $S_{t+1}$: 选择 $A_t$ 后的下一时刻的状态

这个过程的详细分解
- $S_t \rightarrow A_t$: 给定 State 下选择什么样的 Action，$\pi(A_t=a | S_t=s)$
- $S_t,A_t \rightarrow R_{t+1}$: 某个 State 下采取某个 Action 得到什么样的 Reward，$p(R_{t+1}=r | S_t = s, A_t=a)$
- $S_t,A_t \rightarrow S_{t+1}$: 某个 State 下采取某个 Action 会转换成什么样的 State，$p(S_{t+1}=s' | S_t = s, A_t = a)$

扩展到多步
$$S_t \stackrel{A_t}\longrightarrow R_{t+1}，S_{t+1}
\stackrel{A_{t+1}}\longrightarrow R_{t+2}，S_{t+2} \stackrel{A_{t+2}}\longrightarrow R_{t+3}，...$$

那么得到的 Return 就是

$$G_t \doteq \gamma^0R_{t+1} + \gamma^1 R_{t+2} + \gamma^2R_{t+3} + ... = \sum\limits_{k=0}^{\infty}\gamma^kR_{t+k+1}$$



&emsp;
>Value Function（价值函数）
- 价值函数是 State、或者 State 与 Action 二元组的函数，用来评估当前 Agent 在给定状态（或给定状态与动作）下 "有多好"
- "有多好": 指 Return 的期望值
- 所以，Return 用来评价一个 Policy 的好坏


&emsp;
>Policy（策略）
- Policy 是从状态（State）到每个动作（Action）的选择概率的映射
- $\pi(a|s)$: 当 $S_t=s$ 时，选择 $A_t=a$ 的概率
- 每个 $s\in \mathcal{S}$ 都定义了一个在 $a\in \mathcal{A}$ 上的概率分布


&emsp;
# 1 State-Value function for policy $\pi$



>策略 $\pi$ 的状态价值函数
- 把策略 $\pi$ 下状态 $s$ 的价值函数记为 $v_{\pi}(s)$，即从状态 $s$ 开始，Agent 按照策略 $\pi$ 进行决策所获得的 Return 的概率期望值

$$v_{\pi}(s) \doteq \mathbb{E}[G_t | S_t=s] = \mathbb{E}\Big[\sum\limits_{k=0}^{\infty}\gamma^k R_{t+k+1} |S_t = s \Big]，s \in \mathcal{S} $$
- $\mathbb{E}[.]$: 表示给定 Policy（策略）$\pi$ 时，一个随机变量的期望值

- 如果每个遇到的 State 都记录实际 Return 的平均值，那么这个平均值会收敛到 $v_{\pi}(s)$

&emsp;
# 2 Action-Value function for policy $\pi$
>策略 $\pi$ 的动作价值函数
$$q_{\pi}(s,a) \doteq \mathbb{E}_{\pi}[G_t | S_t = s, A_t=a] = \mathbb{E}_{\pi}\Big[ \sum\limits_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_t=s, A_t=a \Big] $$

- 如果为每个 State 的每个 Action 都保留单独的平均值，这些平均值也会收敛到动作价值 $q_{\pi}(s,a)$

&emsp;
# 3 Bellman Equation（贝尔曼方程）
- 用灯饰表达了状态价值和后续状态价值之间的关系
$$v_{\pi}(s) \doteq \mathbb{E}_{\pi}[G_t | S_t=s]$$
$$=\mathbb{E}_{\pi}[R_{t+1} + \gamma G_{t+1} | S_t = s]$$
$$= \sum\limits_{a}\pi(a|s)\sum\limits_{s'}\sum\limits_{r} p(s', r | s,a)\Big[r + \gamma \mathbb{E}_{\pi}[G_{t+1} | S_{t+1} = s'] \Big]$$
$$= \sum\limits_{a}\pi(a|s)\sum\limits_{s', r} p(s', r | s,a)\Big[r + \gamma v_{\pi}(s')\Big]（贝尔曼方程）$$



&emsp;
# 3 讨论

当 Environment 中有很多 State 时，独立的估算每个 State 的平均值是不切实际的。这种情况下，我们可以将价值函数 $v_{\pi}$ 和 $q_{\pi}$ 进行参数化，然后通过调整 $v_{\pi}$ 的参数来更好的计算回报值
