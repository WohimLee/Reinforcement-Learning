&emsp;
# Policy-Value

## 知识回顾
回顾前面讲到的单步 Step 过程：
$$S_t \stackrel{A_t}\longrightarrow R_{t+1}，S_{t+1}$$
- $t，t+1$: Agent 和 Environment 发生交互的 `离散时刻`
- $S_t$: $t$ 时刻的状态 State
- $A_t$: $S_t$ 状态下执行的一个 Action
- $R_{t+1}$: 选择 $A_t$ 后的 Reward
- $S_{t+1}$: 选择 $A_t$ 后的下一时刻的状态

这个过程的详细分解
- $S_t \rightarrow A_t$: 给定 State 下选择什么样的 Action，$\pi(A_t=a | S_t=s)$
- $S_t,A_t \rightarrow R_{t+1}$: 某个 State 下采取某个 Action 得到什么样的 Reward，$p(R_{t+1}=r | S_t = s, A_t=a)$
- $S_t,A_t \rightarrow S_{t+1}$: 某个 State 下采取某个 Action 会转换成什么样的 State，$p(S_{t+1}=s' | S_t = s, A_t = a)$

扩展到多步
$$S_t \stackrel{A_t}\longrightarrow R_{t+1}，S_{t+1}
\stackrel{A_{t+1}}\longrightarrow R_{t+2}，S_{t+2} \stackrel{A_{t+2}}\longrightarrow R_{t+3}，...$$

那么得到的 Return 就是

$$G_t \doteq \gamma^0R_{t+1} + \gamma^1 R_{t+2} + \gamma^2R_{t+3} + ... $$
$$=R_{t+1} + \gamma(R_{t+2} + \gamma R_{t+3}+...)$$
$$R_{t+1}+\gamma G_{t+1}$$



&emsp;
>Value Function（价值函数）
- 价值函数是 State、或者 State 与 Action 二元组的函数，用来评估当前 Agent 在给定状态（或给定状态与动作）下 "有多好"
- "有多好": 指 Return 的期望值
- 所以，Return 用来评价一个 Policy 的好坏


&emsp;
>Policy（策略）
- Policy 表示在状态（State），每个动作（Action）的被选择概率分布
- $\pi(a|s)$: 当 $S_t=s$ 时，选择 $A_t=a$ 的概率
- 每个 $s\in \mathcal{S}$ 都定义了一个在 $a\in \mathcal{A}$ 上的概率分布


&emsp;
# 1 State-Value function for policy $\pi$

>策略 $\pi$ 的状态价值函数
- 考虑所有的 Action 带来的 Reward，然后求 $\mathbb{E}$
- 把策略 $\pi$ 下状态 $s$ 的价值函数记为 $v_{\pi}(s)$，即从状态 $s$ 开始，Agent 按照策略 $\pi$ 进行决策所获得的 Return 的概率期望值

    $$v_{\pi}(s) = \mathbb{E}[G_t | S_t=s] $$
    $$= \mathbb{E}[R_{t+1} + \gamma G_{t+1}| S_t=s]$$
- $G_t$ 分成了两部分：
    - Immediate reward: $R_{t+1}$
    - Future reward: $\gamma  G_{t+1}$

- 把 $\mathbb{E}$ 里面的 `+` 号分开得：
    $$ \mathbb{E}[R_{t+1}  | S_t=s] + \gamma  \mathbb{E}[G_{t+1} | S_t = s]$$
    - $\mathbb{E}[R_{t+1}  | S_t=s]$
    - $\mathbb{E}[G_{t+1} | S_t = s]$


- 如果每个遇到的 State 都记录实际 Return 的平均值，那么这个平均值会收敛到 $v_{\pi}(s)$
